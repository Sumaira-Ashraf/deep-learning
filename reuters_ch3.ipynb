{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ/MPy4srzBNfdLF0ysSCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sumaira-Ashraf/deep-learning/blob/main/reuters_ch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJjnjgBPG5Ha",
        "outputId": "a5e13200-b937-4e04-ac9f-9084bbb528ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
        "\n",
        "#num_words=10000: This limits the vocabulary size to the top 10,000 most frequent words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " len(x_train)"
      ],
      "metadata": {
        "id": "fhFrkUzCAlcN",
        "outputId": "6cfb5088-db83-4f7b-bbff-c7cfb0c380d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "id": "Tqzv4WeqDTKB",
        "outputId": "e61e390a-c532-4093-f910-c6323c37331e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[10]"
      ],
      "metadata": {
        "id": "VuBxOywiDzpo",
        "outputId": "c922508c-b1d1-43ef-dfaa-4fec06595536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Decode the News Articles**"
      ],
      "metadata": {
        "id": "g8igyYJ9JFsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict(\n",
        " [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_newswire = \" \".join(\n",
        " [reverse_word_index.get(i - 3, \"?\") for i in x_train[0]])"
      ],
      "metadata": {
        "id": "U_CCrOv9EWka",
        "outputId": "417f440c-aad8-4531-d8f4-bff3cf69dc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "\u001b[1m550378/550378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[0]), y_train[0])\n",
        "print(len(x_train[1]), y_train[1])\n",
        "print(len(x_train[2]), y_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vAio154JbKT",
        "outputId": "1655face-e051-4694-935b-9a13fb7fe551"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87 3\n",
            "56 4\n",
            "139 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max([max(sequence) for sequence in x_train])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keLNg-3AKJjh",
        "outputId": "93152133-42ad-4a8c-d90a-bf07602759d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train[0] = 67, 84,22,482,26,7,48"
      ],
      "metadata": {
        "id": "NrURU2kEQRJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgL2XZLtQR6g",
        "outputId": "94530da6-33b5-4c8c-bc3d-b13bab18a614"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 2,\n",
              " 8,\n",
              " 43,\n",
              " 10,\n",
              " 447,\n",
              " 5,\n",
              " 25,\n",
              " 207,\n",
              " 270,\n",
              " 5,\n",
              " 3095,\n",
              " 111,\n",
              " 16,\n",
              " 369,\n",
              " 186,\n",
              " 90,\n",
              " 67,\n",
              " 7,\n",
              " 89,\n",
              " 5,\n",
              " 19,\n",
              " 102,\n",
              " 6,\n",
              " 19,\n",
              " 124,\n",
              " 15,\n",
              " 90,\n",
              " 67,\n",
              " 84,\n",
              " 22,\n",
              " 482,\n",
              " 26,\n",
              " 7,\n",
              " 48,\n",
              " 4,\n",
              " 49,\n",
              " 8,\n",
              " 864,\n",
              " 39,\n",
              " 209,\n",
              " 154,\n",
              " 6,\n",
              " 151,\n",
              " 6,\n",
              " 83,\n",
              " 11,\n",
              " 15,\n",
              " 22,\n",
              " 155,\n",
              " 11,\n",
              " 15,\n",
              " 7,\n",
              " 48,\n",
              " 9,\n",
              " 4579,\n",
              " 1005,\n",
              " 504,\n",
              " 6,\n",
              " 258,\n",
              " 6,\n",
              " 272,\n",
              " 11,\n",
              " 15,\n",
              " 22,\n",
              " 134,\n",
              " 44,\n",
              " 11,\n",
              " 15,\n",
              " 16,\n",
              " 8,\n",
              " 197,\n",
              " 1245,\n",
              " 90,\n",
              " 67,\n",
              " 52,\n",
              " 29,\n",
              " 209,\n",
              " 30,\n",
              " 32,\n",
              " 132,\n",
              " 6,\n",
              " 109,\n",
              " 15,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_word_index.get(67-3,'?'))\n",
        "print(reverse_word_index.get(84-3,'?'))\n",
        "print(reverse_word_index.get(22-3,'?'))\n",
        "print(reverse_word_index.get(482-3,'?'))\n",
        "print(reverse_word_index.get(26-3,'?'))\n",
        "print(reverse_word_index.get(7-3,'?'))\n",
        "print(reverse_word_index.get(48-3,'?'))\n",
        "#67, 84,22,482,26,7,48"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RCizU5qQxYV",
        "outputId": "8457103c-56ea-4c28-9076-472f30ec5e70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "share\n",
            "up\n",
            "from\n",
            "70\n",
            "cts\n",
            "in\n",
            "1986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join([reverse_word_index.get(i-3,\"?\") for i in x_train[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "S_tcjVisRtvu",
        "outputId": "b5c1fd4b-46ff-45a2-b9a5-3738e7429f6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "\n",
        "print(decode_review(x_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyBGjskhIOcU",
        "outputId": "163ac349-b3d1-414d-f921-9473e90f7ecc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare the Data for Training:**\n",
        "\n",
        "Tokenization:\n",
        "\n",
        "Breaks text into words or tokens: This process divides the text into individual words or phrases, which are the fundamental units of meaning.\n",
        "\n",
        "Assigns numerical indices to each token: Each unique token is assigned a unique integer, creating a vocabulary.\n",
        "\n",
        "Converts text to numerical sequences: The text documents are then represented as sequences of these numerical indices."
      ],
      "metadata": {
        "id": "o2ICORs0N4vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000)\n",
        "X_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "X_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
      ],
      "metadata": {
        "id": "xORA7wS0JOVL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding:**\n",
        "\n",
        "Creates a binary vector for each token: For each token in the vocabulary, a vector of zeros is created.\n",
        "\n",
        "Sets a single element to 1: The element corresponding to the specific token is set to 1.\n",
        "\n",
        "Represents categorical data: This technique is ideal for representing categorical data, such as words, where each category is mutually exclusive.\n",
        "\n",
        "Handling Categorical Data: One-hot encoding is effective for representing the different news categories in the Reuters dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TfTs_xB8TVJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Assuming y_train holds the raw training labels:\n",
        "y_train_categorical = tf.keras.utils.to_categorical(y_train)\n",
        "y_test_categorical = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Now, y_train_categorical and y_test_categorical hold the one-hot encoded labels.\n",
        "# You can use them in your model training process"
      ],
      "metadata": {
        "id": "ZNrEBaTsLjmR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Model Architecture**\n",
        "\n",
        "Let's break down the provided Keras model architecture step-by-step:\n",
        "\n",
        "**1. Sequential Model:**\n",
        "\n",
        "This is a linear stack of layers. Each layer processes the output of the previous layer.\n",
        "\n",
        "**2. Dense Layers:**\n",
        "\n",
        "First Dense Layer:\n",
        "Units: 64\n",
        "Activation: ReLU (Rectified Linear Unit)\n",
        "Input Shape: (10000,) - This indicates that the input to the model is a 1-dimensional array with 10,000 elements. This is likely the output of a tokenization and one-hot encoding process, where each element represents the presence or absence of a specific word in the vocabulary.\n",
        "Second Dense Layer:\n",
        "Units: 64\n",
        "Activation: ReLU\n",
        "Input Shape: The output of the previous layer.\n",
        "Third Dense Layer:\n",
        "Units: 46\n",
        "Activation: Softmax - This activation function outputs a probability distribution over 46 classes. In the case of the Reuters dataset, these 46 classes represent different news topics.\n",
        "\n",
        "**3. Dropout Layers:**\n",
        "\n",
        "Dropout(0.5): This layer randomly sets 50% of the input units to zero at each update during training. This technique helps prevent overfitting by reducing the model's reliance on specific features.\n",
        "Overall Functionality:\n",
        "\n",
        "Input: The model takes a 10,000-dimensional vector as input, representing a tokenized and one-hot encoded news article.\n",
        "Hidden Layers: The first two dense layers with ReLU activation extract features from the input data. The dropout layers help regularize the model.\n",
        "Output Layer: The final dense layer with softmax activation outputs a probability distribution over 46 classes. The class with the highest probability is predicted as the most likely topic for the input news article.\n",
        "\n",
        "**Why This Architecture for Reuters Dataset?**\n",
        "\n",
        "Dense Layers: Suitable for capturing complex relationships between features.\n",
        "ReLU Activation: Introduces non-linearity, enabling the model to learn intricate patterns.\n",
        "Dropout: Prevents overfitting by randomly dropping out neurons during training.\n",
        "Softmax Activation: Outputs a probability distribution over multiple classes, making it ideal for multi-class classification tasks.\n",
        "This architecture is well-suited for text classification tasks like the Reuters dataset, where the goal is to categorize news articles into different topics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GwqL0mU4VO6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[1000:]"
      ],
      "metadata": {
        "id": "RJHgihIjJQYz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10000,)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(46, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Use the one-hot encoded labels for training and validation\n",
        "model.fit(X_train, y_train_categorical, epochs=20, batch_size=32, validation_data=(X_test, y_test_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBcX-yJyLxpg",
        "outputId": "35bb8ae0-8333-40fa-9117-7306cd6c7591"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.3929 - loss: 2.5420 - val_accuracy: 0.6866 - val_loss: 1.3714\n",
            "Epoch 2/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.6839 - loss: 1.3520 - val_accuracy: 0.7155 - val_loss: 1.1934\n",
            "Epoch 3/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.7293 - loss: 1.1101 - val_accuracy: 0.7440 - val_loss: 1.1156\n",
            "Epoch 4/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7674 - loss: 0.9500 - val_accuracy: 0.7667 - val_loss: 1.0685\n",
            "Epoch 5/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.7797 - loss: 0.8786 - val_accuracy: 0.7703 - val_loss: 1.0490\n",
            "Epoch 6/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7936 - loss: 0.7952 - val_accuracy: 0.7738 - val_loss: 1.0534\n",
            "Epoch 7/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8094 - loss: 0.7357 - val_accuracy: 0.7765 - val_loss: 1.0819\n",
            "Epoch 8/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8257 - loss: 0.6608 - val_accuracy: 0.7792 - val_loss: 1.0823\n",
            "Epoch 9/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8369 - loss: 0.6095 - val_accuracy: 0.7863 - val_loss: 1.0957\n",
            "Epoch 10/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8552 - loss: 0.5562 - val_accuracy: 0.7921 - val_loss: 1.1487\n",
            "Epoch 11/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8466 - loss: 0.5609 - val_accuracy: 0.7939 - val_loss: 1.1223\n",
            "Epoch 12/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8527 - loss: 0.5279 - val_accuracy: 0.7947 - val_loss: 1.1708\n",
            "Epoch 13/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8540 - loss: 0.5177 - val_accuracy: 0.7961 - val_loss: 1.1863\n",
            "Epoch 14/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8630 - loss: 0.4832 - val_accuracy: 0.7916 - val_loss: 1.2271\n",
            "Epoch 15/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8648 - loss: 0.4970 - val_accuracy: 0.7930 - val_loss: 1.2217\n",
            "Epoch 16/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8726 - loss: 0.4518 - val_accuracy: 0.7974 - val_loss: 1.2491\n",
            "Epoch 17/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8765 - loss: 0.4442 - val_accuracy: 0.8019 - val_loss: 1.2587\n",
            "Epoch 18/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.8761 - loss: 0.4397 - val_accuracy: 0.7934 - val_loss: 1.3292\n",
            "Epoch 19/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.8826 - loss: 0.4197 - val_accuracy: 0.7956 - val_loss: 1.3292\n",
            "Epoch 20/20\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.8851 - loss: 0.3920 - val_accuracy: 0.7965 - val_loss: 1.4015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d1d33b5d930>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assuming x_train is a list or contains lists, convert it to a NumPy array with object dtype first\n",
        "x_train = np.array(x_train, dtype=object)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_train = pad_sequences(x_train)  # Padding with default values (0)\n",
        "\n",
        "# Now you can safely convert to float32\n",
        "x_train = x_train.astype(np.float32)\n",
        "\n",
        "# Apply the same preprocessing to x_test:\n",
        "x_test = np.array(x_test, dtype=object)  # Convert to NumPy array\n",
        "x_test = pad_sequences(x_test, maxlen=x_train.shape[1])  # Pad to the same length as x_train\n",
        "x_test = x_test.astype(np.float32)  # Convert to float32\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(46, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train_categorical, epochs=9, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test_categorical)"
      ],
      "metadata": {
        "id": "ilwLtOAOKQxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}